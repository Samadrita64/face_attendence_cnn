{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4347ddf0-5dc0-4551-822d-7b7a577af291",
   "metadata": {},
   "source": [
    "face recognization useing cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9798ce0-edba-444b-a024-c3d061a7c4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìå Step 1: Import Libraries\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b0f81de-6d5b-4c4c-91c9-0fd8bbe0fc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è±Ô∏è 10 seconds passed. Stopping capture.\n",
      "‚úÖ Camera released and windows closed.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import time\n",
    "\n",
    "# üìå Change this to the person's name\n",
    "name = \"\"\n",
    "\n",
    "# üìÅ Create directory for that person's images\n",
    "data_dir = f\"face_attendance/dataset/{name}\"\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# üì∑ Open webcam\n",
    "cam = cv2.VideoCapture(0)\n",
    "cv2.namedWindow(\"Capture Faces\")\n",
    "\n",
    "img_count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    while img_count < 100:\n",
    "        ret, frame = cam.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        cv2.imshow(\"Capture Faces\", frame)\n",
    "\n",
    "        # Auto-capture every 0.5 sec or wait for SPACE key\n",
    "        k = cv2.waitKey(1)\n",
    "        if k % 256 == 32:  # Press SPACE to capture\n",
    "            img_path = f\"{data_dir}/{img_count}.jpg\"\n",
    "            cv2.imwrite(img_path, frame)\n",
    "            print(f\"[INFO] {img_path} saved.\")\n",
    "            img_count += 1\n",
    "\n",
    "        # Stop after 10 seconds\n",
    "        if time.time() - start_time > 10:\n",
    "            print(\"‚è±Ô∏è 10 seconds passed. Stopping capture.\")\n",
    "            break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"‚õî Interrupted by user.\")\n",
    "\n",
    "finally:\n",
    "    cam.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"‚úÖ Camera released and windows closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb966811-f0e0-4fa5-acd0-f5103df90b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Found dataset folder\n",
      "üîç Processing: Ankita\n",
      "üîç Processing: Aritra\n",
      "üîç Processing: Johisa\n",
      "üîç Processing: Samadrita\n",
      "üìä Total images: 37\n",
      "üë§ Classes found: {0: 'Ankita', 1: 'Aritra', 2: 'Johisa', 3: 'Samadrita'}\n",
      "‚úÖ Dataset loaded and split successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "label_dict = {}\n",
    "\n",
    "base_path = \"face_attendance/dataset/\"\n",
    "label_id = 0\n",
    "\n",
    "if not os.path.exists(base_path):\n",
    "    print(\"‚ùå Dataset folder not found!\")\n",
    "else:\n",
    "    print(\"üìÅ Found dataset folder\")\n",
    "\n",
    "for person in os.listdir(base_path):\n",
    "    person_path = os.path.join(base_path, person)\n",
    "    if not os.path.isdir(person_path):\n",
    "        print(f\"‚ö†Ô∏è Skipping non-folder item: {person}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"üîç Processing: {person}\")\n",
    "    label_dict[label_id] = person\n",
    "\n",
    "    for img_name in os.listdir(person_path):\n",
    "        img_path = os.path.join(person_path, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            print(f\"‚ö†Ô∏è Skipped unreadable image: {img_path}\")\n",
    "            continue\n",
    "\n",
    "        img = cv2.resize(img, (100, 100))\n",
    "        data.append(img)\n",
    "        labels.append(label_id)\n",
    "\n",
    "    if label_id not in label_dict:\n",
    "        print(f\"‚ö†Ô∏è No valid images found for: {person}\")\n",
    "    label_id += 1\n",
    "\n",
    "# Confirm what's been loaded\n",
    "print(f\"üìä Total images: {len(data)}\")\n",
    "print(f\"üë§ Classes found: {label_dict}\")\n",
    "\n",
    "if len(data) == 0:\n",
    "    raise ValueError(\"üö´ No valid image data found. Check your dataset folders.\")\n",
    "\n",
    "# Preprocess\n",
    "data = np.array(data) / 255.0\n",
    "labels = to_categorical(labels)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, labels, test_size=0.2, random_state=42\n",
    ")\n",
    "print(\"‚úÖ Dataset loaded and split successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18fca572-56e5-4832-a5c4-4106b7fedc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Ankita: 8 images\n",
      "üìÅ Aritra: 10 images\n",
      "üìÅ Johisa: 8 images\n",
      "üìÅ Samadrita: 11 images\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "dataset_path = 'face_attendance/dataset'\n",
    "classes = sorted(os.listdir(dataset_path))\n",
    "\n",
    "for name in classes:\n",
    "    img_count = len(os.listdir(os.path.join(dataset_path, name)))\n",
    "    print(f\"üìÅ {name}: {img_count} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be593a4d-c544-4fc9-abb8-b71c936b8224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Class distribution: Counter({'Samadrita': 11, 'Aritra': 10, 'Ankita': 8, 'Johisa': 8})\n",
      "‚úÖ Final label_dict used: {0: 'Ankita', 1: 'Aritra', 2: 'Johisa', 3: 'Samadrita'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.1724 - loss: 1.3917 - val_accuracy: 0.3750 - val_loss: 1.7797\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 359ms/step - accuracy: 0.2759 - loss: 1.8876 - val_accuracy: 0.3750 - val_loss: 1.8146\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 422ms/step - accuracy: 0.4483 - loss: 1.4685 - val_accuracy: 0.2500 - val_loss: 1.9072\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 406ms/step - accuracy: 0.5172 - loss: 1.3871 - val_accuracy: 0.1250 - val_loss: 1.7395\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 374ms/step - accuracy: 0.5862 - loss: 1.2187 - val_accuracy: 0.1250 - val_loss: 1.4543\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 439ms/step - accuracy: 0.7241 - loss: 0.9952 - val_accuracy: 0.6250 - val_loss: 1.2244\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 415ms/step - accuracy: 0.9310 - loss: 0.8678 - val_accuracy: 0.5000 - val_loss: 1.1352\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 423ms/step - accuracy: 0.7586 - loss: 0.8469 - val_accuracy: 0.5000 - val_loss: 1.1003\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 460ms/step - accuracy: 0.7586 - loss: 0.7510 - val_accuracy: 0.7500 - val_loss: 1.0719\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 389ms/step - accuracy: 0.9310 - loss: 0.6014 - val_accuracy: 0.6250 - val_loss: 1.0633\n",
      "‚úÖ Model saved successfully!\n",
      "‚úÖ label_dict.pkl saved: ['Ankita', 'Aritra', 'Johisa', 'Samadrita']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "\n",
    "# ------------------- Load Dataset -------------------\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "dataset_path = \"face_attendance/dataset\"\n",
    "all_folders = sorted(os.listdir(dataset_path))\n",
    "\n",
    "for name in all_folders:\n",
    "    person_folder = os.path.join(dataset_path, name)\n",
    "    if not os.path.isdir(person_folder):\n",
    "        continue\n",
    "\n",
    "    image_count = 0\n",
    "    for img_name in os.listdir(person_folder):\n",
    "        img_path = os.path.join(person_folder, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (100, 100))\n",
    "            data.append(img)\n",
    "            labels.append(name)\n",
    "            image_count += 1\n",
    "\n",
    "    if image_count == 0:\n",
    "        print(f\"‚ö†Ô∏è Skipped '{name}' (no valid images)\")\n",
    "\n",
    "# Check data balance\n",
    "print(\"üìä Class distribution:\", Counter(labels))\n",
    "\n",
    "data = np.array(data) / 255.0\n",
    "labels = np.array(labels)\n",
    "\n",
    "# ------------------- Encode Labels -------------------\n",
    "le = LabelEncoder()\n",
    "labels_encoded = le.fit_transform(labels)\n",
    "labels_cat = to_categorical(labels_encoded)\n",
    "\n",
    "# Create label_dict from fitted encoder\n",
    "label_dict = {i: name for i, name in enumerate(le.classes_)}\n",
    "print(\"‚úÖ Final label_dict used:\", label_dict)\n",
    "\n",
    "# ------------------- Train-Test Split -------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels_cat, test_size=0.2, random_state=42)\n",
    "\n",
    "# ------------------- Build Model -------------------\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(100,100,3)),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(len(label_dict), activation='softmax')  # Matches number of classes\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# ------------------- Train -------------------\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10)\n",
    "\n",
    "# ------------------- Save Model and Labels -------------------\n",
    "os.makedirs(\"face_attendance/model\", exist_ok=True)\n",
    "\n",
    "model.save(\"face_attendance/model/facenet_cnn.keras\")\n",
    "print(\"‚úÖ Model saved successfully!\")\n",
    "\n",
    "with open(\"face_attendance/model/label_dict.pkl\", \"wb\") as f:\n",
    "    pickle.dump(le.classes_.tolist(), f)\n",
    "print(\"‚úÖ label_dict.pkl saved:\", le.classes_.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cad79d9d-8146-4d85-b0ee-4ff5a5e6353a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Found classes: {0: 'Ankita', 1: 'Aritra', 2: 'Johisa', 3: 'Samadrita'}\n"
     ]
    }
   ],
   "source": [
    "print(\"‚úÖ Found classes:\", label_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dff7a528-405e-401f-af06-74c42061b8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded label_dict from training: ['Ankita', 'Aritra', 'Johisa', 'Samadrita']\n",
      "üîé Detected 0 face(s)\n",
      "üîé Detected 0 face(s)\n",
      "üîé Detected 0 face(s)\n",
      "üîé Detected 0 face(s)\n",
      "üîé Detected 0 face(s)\n",
      "üîé Detected 1 face(s)\n",
      "üß† Prediction: [0.12411263 0.27277166 0.00092601 0.6021897 ]\n",
      "üîç Top Label: Samadrita | Confidence: 0.60 | Margin: 0.33\n",
      "üîé Detected 1 face(s)\n",
      "üß† Prediction: [0.10725756 0.15930666 0.00161284 0.7318229 ]\n",
      "üîç Top Label: Samadrita | Confidence: 0.73 | Margin: 0.57\n",
      "üîé Detected 1 face(s)\n",
      "üß† Prediction: [0.0895171  0.1399293  0.00356399 0.76698965]\n",
      "üîç Top Label: Samadrita | Confidence: 0.77 | Margin: 0.63\n",
      "üîé Detected 1 face(s)\n",
      "üß† Prediction: [0.06464674 0.11951101 0.00443397 0.81140834]\n",
      "üîç Top Label: Samadrita | Confidence: 0.81 | Margin: 0.69\n",
      "üîé Detected 1 face(s)\n",
      "üß† Prediction: [0.11205483 0.12377857 0.00399971 0.7601669 ]\n",
      "üîç Top Label: Samadrita | Confidence: 0.76 | Margin: 0.64\n",
      "üîé Detected 1 face(s)\n",
      "üß† Prediction: [0.08990379 0.10343802 0.00306601 0.8035922 ]\n",
      "üîç Top Label: Samadrita | Confidence: 0.80 | Margin: 0.70\n",
      "üîé Detected 1 face(s)\n",
      "üß† Prediction: [0.1458183  0.11064692 0.00310971 0.74042505]\n",
      "üîç Top Label: Samadrita | Confidence: 0.74 | Margin: 0.59\n",
      "üîé Detected 1 face(s)\n",
      "üß† Prediction: [0.1601839  0.1103613  0.00281629 0.72663856]\n",
      "üîç Top Label: Samadrita | Confidence: 0.73 | Margin: 0.57\n",
      "üîé Detected 1 face(s)\n",
      "üß† Prediction: [0.14738804 0.11362074 0.00299947 0.73599184]\n",
      "üîç Top Label: Samadrita | Confidence: 0.74 | Margin: 0.59\n",
      "üîé Detected 1 face(s)\n",
      "üß† Prediction: [0.14538607 0.10857227 0.00268471 0.7433569 ]\n",
      "üîç Top Label: Samadrita | Confidence: 0.74 | Margin: 0.60\n",
      "üîé Detected 1 face(s)\n",
      "üß† Prediction: [0.18690768 0.13404456 0.00211723 0.6769305 ]\n",
      "üîç Top Label: Samadrita | Confidence: 0.68 | Margin: 0.49\n",
      "üîé Detected 1 face(s)\n",
      "üß† Prediction: [0.1893025  0.14877898 0.00208287 0.6598356 ]\n",
      "üîç Top Label: Samadrita | Confidence: 0.66 | Margin: 0.47\n",
      "üîé Detected 1 face(s)\n",
      "üß† Prediction: [0.12006038 0.11998028 0.00246458 0.7574948 ]\n",
      "üîç Top Label: Samadrita | Confidence: 0.76 | Margin: 0.64\n",
      "üîé Detected 1 face(s)\n",
      "üß† Prediction: [0.17721276 0.13482715 0.00225329 0.68570673]\n",
      "üîç Top Label: Samadrita | Confidence: 0.69 | Margin: 0.51\n",
      "üîé Detected 1 face(s)\n",
      "üß† Prediction: [0.14303234 0.13288245 0.00247471 0.7216104 ]\n",
      "üîç Top Label: Samadrita | Confidence: 0.72 | Margin: 0.58\n",
      "üîé Detected 1 face(s)\n",
      "üß† Prediction: [0.12703939 0.13769667 0.00262994 0.732634  ]\n",
      "üîç Top Label: Samadrita | Confidence: 0.73 | Margin: 0.59\n",
      "üîé Detected 1 face(s)\n",
      "üß† Prediction: [0.10666984 0.12965786 0.00328562 0.7603867 ]\n",
      "üîç Top Label: Samadrita | Confidence: 0.76 | Margin: 0.63\n",
      "üîé Detected 1 face(s)\n",
      "üß† Prediction: [0.03747904 0.09851772 0.0046659  0.85933733]\n",
      "üîç Top Label: Samadrita | Confidence: 0.86 | Margin: 0.76\n",
      "üîé Detected 1 face(s)\n",
      "üß† Prediction: [0.08941744 0.16318893 0.00513708 0.7422566 ]\n",
      "üîç Top Label: Samadrita | Confidence: 0.74 | Margin: 0.58\n",
      "üîé Detected 0 face(s)\n",
      "üîé Detected 1 face(s)\n",
      "üß† Prediction: [7.2279410e-03 4.8944507e-02 8.2826946e-04 9.4299930e-01]\n",
      "üîç Top Label: Samadrita | Confidence: 0.94 | Margin: 0.89\n",
      "üîé Detected 0 face(s)\n",
      "üîé Detected 0 face(s)\n",
      "üîé Detected 0 face(s)\n",
      "üîé Detected 0 face(s)\n",
      "üîé Detected 0 face(s)\n",
      "üîé Detected 0 face(s)\n",
      "üîé Detected 0 face(s)\n",
      "üîé Detected 0 face(s)\n",
      "üîé Detected 0 face(s)\n",
      "üîé Detected 0 face(s)\n",
      "üîé Detected 0 face(s)\n",
      "üîé Detected 0 face(s)\n",
      "üîé Detected 0 face(s)\n",
      "üîé Detected 0 face(s)\n",
      "üîé Detected 1 face(s)\n",
      "üß† Prediction: [3.3470844e-03 3.8789086e-02 5.3217862e-04 9.5733166e-01]\n",
      "üîç Top Label: Samadrita | Confidence: 0.96 | Margin: 0.92\n",
      "üîé Detected 0 face(s)\n",
      "üîé Detected 0 face(s)\n",
      "üîé Detected 0 face(s)\n",
      "üîé Detected 0 face(s)\n",
      "üîé Detected 0 face(s)\n",
      "üîé Detected 0 face(s)\n",
      "üîé Detected 0 face(s)\n",
      "üîé Detected 0 face(s)\n",
      "üîé Detected 0 face(s)\n",
      "üîé Detected 0 face(s)\n",
      "üîé Detected 0 face(s)\n",
      "üîé Detected 0 face(s)\n",
      "üîé Detected 0 face(s)\n",
      "üîé Detected 0 face(s)\n",
      "üîé Detected 0 face(s)\n",
      "üîé Detected 0 face(s)\n",
      "üîé Detected 0 face(s)\n",
      "üîé Detected 0 face(s)\n",
      "üîé Detected 0 face(s)\n",
      "üîé Detected 0 face(s)\n",
      "üîé Detected 0 face(s)\n",
      "üîé Detected 0 face(s)\n",
      "‚è±Ô∏è 10 seconds passed. Stopping capture.\n",
      "‚úÖ Camera released and windows closed.\n",
      "üìã Attendance Summary:\n",
      " - ‚úÖ Samadrita\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "# ------------------- Load Model & Label Dictionary -------------------\n",
    "model = load_model(\"face_attendance/model/facenet_cnn.keras\")\n",
    "with open(\"face_attendance/model/label_dict.pkl\", \"rb\") as f:\n",
    "    label_dict = pickle.load(f)  # e.g., ['Aritra', 'Johisa', 'Samadrita']\n",
    "print(\"‚úÖ Loaded label_dict from training:\", label_dict)\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# ------------------- Attendance Setup -------------------\n",
    "recognized = set()\n",
    "marked_names = []\n",
    "start_time = time.time()\n",
    "\n",
    "# Thresholds\n",
    "CONFIDENCE_THRESHOLD = 0.55  # stricter confidence\n",
    "MARGIN_THRESHOLD = 0.10      # bigger gap between top 2 predictions\n",
    "\n",
    "def prewhiten(x):\n",
    "    mean = np.mean(x)\n",
    "    std = np.std(x)\n",
    "    std_adj = np.maximum(std, 1.0/np.sqrt(x.size))\n",
    "    y = (x - mean) / std_adj\n",
    "    return y\n",
    "\n",
    "def mark_attendance(name):\n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    file_path = 'face_attendance/attendance.csv'\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        pd.DataFrame(columns=['Name', 'Time']).to_csv(file_path, index=False)\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "    if name not in df['Name'].values:\n",
    "        new_row = pd.DataFrame({'Name': [name], 'Time': [dt_string]})\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(f\"‚úÖ Marked attendance for {name} at {dt_string}\")\n",
    "\n",
    "# ------------------- Real-Time Detection -------------------\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"‚ùå Failed to grab frame\")\n",
    "            break\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "        print(f\"üîé Detected {len(faces)} face(s)\")\n",
    "\n",
    "        for x, y, w, h in faces:\n",
    "            face_img = frame[y:y+h, x:x+w]\n",
    "            face_img = cv2.resize(face_img, (100, 100)) / 255.0\n",
    "            face_img = prewhiten(face_img)\n",
    "            face_img = np.expand_dims(face_img, axis=0)\n",
    "\n",
    "            pred = model.predict(face_img, verbose=0)[0]\n",
    "            label = np.argmax(pred)\n",
    "            confidence = pred[label]\n",
    "            top2 = np.sort(pred)[-2:]\n",
    "            margin = top2[-1] - top2[-2]\n",
    "\n",
    "            # Debug logs\n",
    "            print(f\"üß† Prediction: {pred}\")\n",
    "            print(f\"üîç Top Label: {label_dict[label]} | Confidence: {confidence:.2f} | Margin: {margin:.2f}\")\n",
    "\n",
    "            if confidence >= CONFIDENCE_THRESHOLD and margin >= MARGIN_THRESHOLD:\n",
    "                name = label_dict[label]\n",
    "                display_name = f\"{name} ({confidence:.2f})\"\n",
    "\n",
    "                if name not in recognized:\n",
    "                    mark_attendance(name)\n",
    "                    recognized.add(name)\n",
    "                    marked_names.append(name)\n",
    "            else:\n",
    "                name = \"Unknown\"\n",
    "                display_name = \"Unknown\"\n",
    "                print(f\"üö´ Rejected: Low confidence or margin | Confidence: {confidence:.2f} | Margin: {margin:.2f}\")\n",
    "\n",
    "            # Draw rectangle and label\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, display_name, (x, y - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "\n",
    "        cv2.imshow(\"üì∏ Face Attendance System\", frame)\n",
    "\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            print(\"üõë Quit key pressed.\")\n",
    "            break\n",
    "        if time.time() - start_time > 10:\n",
    "            print(\"‚è±Ô∏è 10 seconds passed. Stopping capture.\")\n",
    "            break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"‚õî Interrupted by user.\")\n",
    "\n",
    "finally:\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(\"‚úÖ Camera released and windows closed.\")\n",
    "\n",
    "    # üìã Summary\n",
    "    if marked_names:\n",
    "        print(\"üìã Attendance Summary:\")\n",
    "        for name in marked_names:\n",
    "            print(f\" - ‚úÖ {name}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No one was marked present.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd00e9bd-12f8-466a-80c3-18e15f96e99e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (2686260890.py, line 65)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[10], line 65\u001b[1;36m\u001b[0m\n\u001b[1;33m    cv2.rectangle(frame, (\u001b[0m\n\u001b[1;37m                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f06df3-d089-4cdb-ab26-a2a585e93471",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
